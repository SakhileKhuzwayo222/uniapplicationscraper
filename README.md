ğŸ“ DHET Institution Scraper & Data Enrichment Pipeline
Overview

The DHET Institution Scraper is a data extraction and enrichment pipeline built to automatically source, clean, and structure data on South African universities and TVET colleges directly from the official Department of Higher Education and Training (DHET) map interface.

The goal of this project is to serve as the data foundation for the MatricLink system â€” enabling users to access accurate, validated, and enriched institutional data for use in education-related applications.

ğŸ§­ Project Purpose

South Africaâ€™s higher education landscape lacks easily accessible structured data for use in modern applications.
This scraper automates that process by:

Extracting all institution names directly from the DHET interactive map.

Cleaning and validating the extracted data.

Optionally enriching institution data with URLs, types, and logos using intelligent inference (Mistral AI integration planned).

Exporting a structured JSON file (sources.json) that can feed into downstream systems or APIs.

ğŸ—ï¸ System Architecture
uniapplicationscraper/
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ dhet_map_scraper.py        # Extracts institution names from DHET map
â”‚   â”œâ”€â”€ scraper.py                 # Fetches and enriches data for each institution
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                  # Provides color-coded console logging
â”‚   â”œâ”€â”€ cleaner.py                 # (Optional) Cleans and validates data
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sources.json               # Output JSON file of all scraped institutions
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper_outputs.py    # Validates scraper output and JSON integrity
â”‚
â”œâ”€â”€ venv/                          # Python virtual environment (ignored in git)
â”‚
â””â”€â”€ README_WebScraper.md           # This file

âš™ï¸ Key Features
ğŸ”¹ 1. Automated Institution Extraction

The scraper navigates the official DHET map (https://www.dhet.gov.za/SitePages/Map.aspx) using Selenium WebDriver to extract every educational marker (universities and TVET colleges).

ğŸ”¹ 2. Dynamic Content Handling

Handles dynamically loaded content (JavaScript-driven map) using Seleniumâ€™s wait conditions and safe interaction with each map marker.

ğŸ”¹ 3. Structured Output

Outputs all results as structured JSON data under data/sources.json, sorted alphabetically and easily parsable by other systems.

ğŸ”¹ 4. Intelligent Data Enrichment (Planned)

Integrates with Mistral AI for automatic enrichment â€” including identifying missing logos, website URLs, and correct institution categories.

ğŸ”¹ 5. Modular Design

Each part of the pipeline (scraping, cleaning, enrichment, validation) is built as an independent, reusable Python module.

ğŸ’» Installation Guide
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/uniapplicationscraper.git
cd uniapplicationscraper

2ï¸âƒ£ Set Up a Virtual Environment
python -m venv venv
source venv/bin/activate     # On Mac/Linux
venv\Scripts\activate        # On Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


Example dependencies include:

selenium
beautifulsoup4
requests
pandas
numpy

4ï¸âƒ£ Set ChromeDriver Path

In dhet_map_scraper.py, update this line with your local path:

CHROMEDRIVER_PATH = "C:/path/to/chromedriver.exe"

ğŸš€ Usage
ğŸ”¸ Run the DHET Map Scraper
python scrapers/dhet_map_scraper.py


â¡ Extracts all institution names and saves them into data/sources.json.

ğŸ”¸ Run the Data Enricher / Web Info Scraper
python scrapers/scraper.py


â¡ Uses sources.json to fetch more data (logos, types, websites) for each institution.

ğŸ”¸ Test Data Integrity
python tests/test_scraper_outputs.py


â¡ Checks that the JSON file exists, is non-empty, valid, and correctly structured.

ğŸ§© Core Functions
File	Function	Purpose
dhet_map_scraper.py	scrape_dhet_institutions()	Extracts institution names from DHET map markers
	get_marker_name()	Interacts safely with map popups to extract text
	setup_driver()	Configures a headless Selenium ChromeDriver
scraper.py	get_institution_info(url)	Fetches title and logo metadata from institution websites
	main()	Coordinates data scraping and enrichment
logger.py	setup_logger()	Provides color-coded and timestamped log output
test_scraper_outputs.py	test_sources_json()	Validates and verifies sources.json integrity
ğŸ§  Planned AI Integration: Mistral

A future phase introduces Mistral integration for intelligent enrichment and validation:

Automatic Classification (University, TVET, Private)

Domain Guessing (detect correct institution URL using model reasoning)

Metadata Completion (logo, motto, city, province inference)

Example prompt for enrichment:

prompt = f"""
Given this institution name: "{name}", return structured JSON with:
- verified official website
- institution type
- logo URL
- location (city, province)
"""

ğŸ§¾ Output Format Example
[
  {
    "name": "University of Johannesburg",
    "type": "University",
    "url": "https://www.uj.ac.za/",
    "logo": "https://www.uj.ac.za/favicon.ico"
  },
  {
    "name": "Sedibeng TVET College",
    "type": "TVET College",
    "url": "https://www.sedcol.co.za/",
    "logo": "https://www.sedcol.co.za/favicon.ico"
  }
]

ğŸ§ª Testing and Validation

All tests can be run independently using:

pytest tests/


Or manually:

python tests/test_scraper_outputs.py


âœ… The test checks:

File existence

Non-empty content

JSON validity

Correct data structure

ğŸ§­ Future Enhancements

 AI enrichment using Mistral

 Integration with MatricLink Backend (C#) API

 Parallel scraping using asyncio or concurrent.futures

 Automatic database sync (PostgreSQL or MongoDB)

 Web dashboard for visualizing scrape progress

ğŸ‘¨â€ğŸ’» Author

Developer: Sakhumuzi Khuzwayo
Project: MatricLink Data Pipeline
Purpose: Provide verified, structured data for educational applications in South Africa.
